{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import urllib\n",
    "import numpy as np\n",
    "import PIL\n",
    "import mne\n",
    "import helper as h\n",
    "import functionss as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_h = mne.io.read_raw_edf('/home/ubuntu/Diffusion/own_diffusion/test_EEGs/H S10 EC.edf', preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_h.drop_channels(\n",
    "                    [\n",
    "                        \"EEG A2-A1\",\n",
    "                        \"EEG 23A-23R\",\n",
    "                        \"EEG 24A-24R\",\n",
    "                        \"EEG T6-LE\",\n",
    "                        \"EEG Cz-LE\",\n",
    "                        \"EEG Pz-LE\",\n",
    "                    ],\n",
    "                    on_missing=\"ignore\",\n",
    "                )\n",
    "raw_h = raw_h.rename_channels(\n",
    "                    {\n",
    "                        \"EEG Fp1-LE\": \"Fp1\",\n",
    "                        \"EEG F3-LE\": \"F3\",\n",
    "                        \"EEG C3-LE\": \"C3\",\n",
    "                        \"EEG P3-LE\": \"P3\",\n",
    "                        \"EEG O1-LE\": \"O1\",\n",
    "                        \"EEG F7-LE\": \"F7\",\n",
    "                        \"EEG Fz-LE\": \"Fz\",\n",
    "                        \"EEG Fp2-LE\": \"Fp2\",\n",
    "                        \"EEG F4-LE\": \"F4\",\n",
    "                        \"EEG C4-LE\": \"C4\",\n",
    "                        \"EEG P4-LE\": \"P4\",\n",
    "                        \"EEG O2-LE\": \"O2\",\n",
    "                        \"EEG F8-LE\": \"F8\",\n",
    "                        \"EEG T3-LE\" : \"T3\",\n",
    "                        \"EEG T5-LE\" : \"T5\",\n",
    "                        \"EEG T4-LE\" : \"T4\",\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names = raw_h.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = raw_h.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_cut = array[:, 1024*6:1024*7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = f.array_to_edf(array_cut, ch_names, 'eeg', 'test.edf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel:\n",
    "\tdef __init__(self, start_shedule=0.0001, end_schedule=0.02, timesteps=300):\n",
    "\t\tself.start_schedule = start_shedule\n",
    "\t\tself.end_schedule = end_schedule\n",
    "\t\tself.timesteps = timesteps\n",
    "\t\t\n",
    "\t\tself.betas = torch.linspace(self.start_schedule, self.end_schedule, self.timesteps)\n",
    "\t\tself.alphas = 1-self.betas\n",
    "\t\tself.alpha_cumprod = torch.cumprod(self.alphas,axis=0)\n",
    "\n",
    "\tdef forward(self, x0, t, device):\n",
    "\t\tnoise = torch.randn_like(x0)/2\n",
    "\t\tsqrt_alphas_cumprod_t = self.get_index_from_list(self.alpha_cumprod.sqrt(), t, x0.shape)\n",
    "\t\tsqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alpha_cumprod), t, x0.shape)\n",
    "\t\tmean = sqrt_alphas_cumprod_t.to(device) * x0.to(device)\n",
    "\t\tvariance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n",
    "\t\treturn mean + variance, noise.to(device)\n",
    "\t\n",
    "\tdef backward(self, x, t, model, **kwargs):\n",
    "\t\t\"\"\"\n",
    "\t\tCalls the model to predict the noise in the image and returns \n",
    "\t\tthe denoised image. \n",
    "\t\tApplies noise to this image, if we are not in the last step yet.\n",
    "\t\t\"\"\"\n",
    "\t\tbetas_t = self.get_index_from_list(self.betas, t, x.shape)\n",
    "\t\tsqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. - self.alpha_cumprod), t, x.shape)\n",
    "\t\tsqrt_recip_alphas_t = self.get_index_from_list(torch.sqrt(1.0 / self.alphas), t, x.shape)\n",
    "\t\tmean = sqrt_recip_alphas_t * (x - betas_t * model(x, t, **kwargs) / sqrt_one_minus_alphas_cumprod_t)\n",
    "\t\tposterior_variance_t = betas_t\n",
    "\n",
    "\t\tif t == 0:\n",
    "\t\t\treturn mean\n",
    "\t\telse:\n",
    "\t\t\tnoise = torch.randn_like(x)\n",
    "\t\t\tvariance = torch.sqrt(posterior_variance_t) * noise \n",
    "\t\t\treturn mean + variance\n",
    "\n",
    "\t\n",
    "\t@staticmethod\n",
    "\tdef get_index_from_list(values, t, x_shape):\n",
    "\t\tbatch_size = x_shape[0]\n",
    "\t\tresult = values.gather(-1,t.cpu())\n",
    "\n",
    "\t\treturn result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = DiffusionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, time_embedding_dims, labels, num_filters = 3, downsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_embedding_dims = time_embedding_dims\n",
    "        self.time_embedding = SinusoidalPositionEmbeddings(time_embedding_dims)\n",
    "        self.labels = labels\n",
    "        if labels:\n",
    "            self.label_mlp = nn.Linear(1, channels_out)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        \n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(channels_in, channels_out, num_filters, padding=1)\n",
    "            self.final = nn.Conv2d(channels_out, channels_out, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(2 * channels_in, channels_out, num_filters, padding=1)\n",
    "            self.final = nn.ConvTranspose2d(channels_out, channels_out, 4, 2, 1)\n",
    "            \n",
    "        self.bnorm1 = nn.BatchNorm2d(channels_out)\n",
    "        self.bnorm2 = nn.BatchNorm2d(channels_out)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, padding=1)\n",
    "        self.time_mlp = nn.Linear(time_embedding_dims, channels_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t, **kwargs):\n",
    "        o = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        o_time = self.relu(self.time_mlp(self.time_embedding(t)))\n",
    "        o = o + o_time[(..., ) + (None, ) * 2]\n",
    "        if self.labels:\n",
    "            label = kwargs.get('labels')\n",
    "            o_label = self.relu(self.label_mlp(label))\n",
    "            o = o + o_label[(..., ) + (None, ) * 2]\n",
    "            \n",
    "        o = self.bnorm2(self.relu(self.conv2(o)))\n",
    "\n",
    "        return self.final(o)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_channels = 1, time_embedding_dims = 128, labels = False, sequence_channels = (64, 128, 256, 512, 1024)):\n",
    "        super().__init__()\n",
    "        self.time_embedding_dims = time_embedding_dims\n",
    "        sequence_channels_rev = reversed(sequence_channels)\n",
    "        \n",
    "        self.downsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels) for channels_in, channels_out in zip(sequence_channels, sequence_channels[1:])])\n",
    "        self.upsampling = nn.ModuleList([Block(channels_in, channels_out, time_embedding_dims, labels,downsample=False) for channels_in, channels_out in zip(sequence_channels[::-1], sequence_channels[::-1][1:])])\n",
    "        self.conv1 = nn.Conv2d(img_channels, sequence_channels[0], 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(sequence_channels[0], img_channels, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x, t, **kwargs):\n",
    "        residuals = []\n",
    "        o = self.conv1(x.float())\n",
    "        for ds in self.downsampling:\n",
    "            o = ds(o, t, **kwargs)\n",
    "            residuals.append(o)\n",
    "        for us, res in zip(self.upsampling, reversed(residuals)):\n",
    "            o = us(torch.cat((o, res), dim=1), t, **kwargs)\n",
    "            \n",
    "        return self.conv2(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(labels=False)\n",
    "unet.to(device)\n",
    "\n",
    "NO_EPOCHS = 500\n",
    "PRINT_FREQUENCY = 40\n",
    "LR = 0.0001\n",
    "BATCH_SIZE = 128\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)\n",
    "VERBOSE= True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_noise_distribution(noise, predicted_noise):\n",
    "    plt.hist(noise.cpu().numpy().flatten(), density = True, alpha = 0.8, label = \"ground truth noise\")\n",
    "    plt.hist(predicted_noise.cpu().numpy().flatten(), density = True, alpha = 0.8, label = \"predicted noise\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_image, min_val, max_val = h.transform_array(array_cut) \n",
    "torch_image = torch_image.unsqueeze(0)\n",
    "torch_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "\tmean_epoch_loss = []\n",
    "\t\n",
    "\tbatch = torch.stack([torch_image] * BATCH_SIZE)\n",
    "\tprint('no crash1')\n",
    "\tt=torch.randint(0,diffusion_model.timesteps,(BATCH_SIZE,)).long().to(device)\n",
    "\tprint('no crash2')\n",
    "\tbatch_noisy_images, noise = diffusion_model.forward(batch, t, device)\n",
    "\tprint('no crash3')\n",
    "\tprint(t.size())\n",
    "\tprint('no crash4')\n",
    "\tprint(batch_noisy_images.size())\n",
    "\tprint('no crash5')\n",
    "\tpredicted_noise = unet(batch_noisy_images, t)\n",
    "\tprint('no crash6')\n",
    "\t\n",
    "\toptimizer.zero_grad()\n",
    "\tprint('no crash7')\n",
    "\tloss=torch.nn.functional.mse_loss(predicted_noise.float(), noise.float())\n",
    "\tprint('no crash8')\n",
    "\tmean_epoch_loss.append(loss.item())\n",
    "\tprint('no crash9')\n",
    "\tloss.backward()\n",
    "\tprint('no crash10')\n",
    "\toptimizer.step()\n",
    "\tprint('no crash11')\n",
    "\t\n",
    "\tif epoch % PRINT_FREQUENCY == 0:\n",
    "\t\tprint('---')\n",
    "\t\tprint(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)}\")\n",
    "\t\tif VERBOSE:\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\tplot_noise_distribution(noise, predicted_noise)\n",
    "\t\t\t\tprint('no crash12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save unet \n",
    "torch.save(unet, 'unet_conv2d_overfitting_HS10_dec 24_28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import io\n",
    "unet = torch.load('unet_conv2d_overfitting_HS10_dec 24_28')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = []\n",
    "    raw_list = []\n",
    "    img = torch.randn(1, 1, 16, 1024).to(device)\n",
    "    for i in reversed(range(diffusion_model.timesteps)):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
    "        img = diffusion_model.backward(img, t, unet.eval())\n",
    "        if i % 50 == 0:\n",
    "            imgs.append(img[0])\n",
    "            #make from 1 16 1024 to 16 1024\n",
    "            print (img[0].shape)\n",
    "            array = img[0].view(16, 1024)\n",
    "            raw_array=h.reverse_transform_array(array, min_val, max_val,changed_shape=False)\n",
    "            raw=f.array_to_edf(raw_array,ch_names,'eeg','test.edf')\n",
    "            raw_list.append(raw)\n",
    "            raw.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_list[-1].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(imgs[5] - raw.get_data()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_psd(raw_list):\n",
    "    for raw in raw_list:\n",
    "        try:\n",
    "            spectrum = raw.compute_psd()\n",
    "        except :\n",
    "            print('ok')\n",
    "        spectrum.plot(average=True, picks=\"data\", exclude=\"bads\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_psd(raw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_psd([raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save img[-1] as fif\n",
    "raw_list[-1].save('/home/ubuntu/Diffusion/made_eegs/overfitted_conv2d_HS10_sec24_28.fif', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
